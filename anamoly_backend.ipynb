{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0c1507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import OneClassSVM\n",
    "from io import BytesIO\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec21c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second cell - Function to process uploaded file\n",
    "def process_traffic_data(file_path):\n",
    "    \"\"\"\n",
    "    Process traffic data from the uploaded CSV file\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print basic info\n",
    "    print(\"Dataset Preview:\")\n",
    "    print(df.head())\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Convert Time to timestamp format\n",
    "    if 'Time' in df.columns:\n",
    "        start_time = datetime.now() - timedelta(hours=24)  # Assume log starts 24 hours ago\n",
    "        df[\"Timestamp\"] = df[\"Time\"].apply(lambda x: start_time + timedelta(seconds=x))\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    original_count = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {original_count - len(df)} duplicate records.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eac7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third cell - Generate traffic analysis\n",
    "def generate_traffic_analysis(df):\n",
    "    \"\"\"\n",
    "    Generate traffic analysis at different time intervals\n",
    "    \"\"\"\n",
    "    # Ensure Timestamp is in datetime format\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # Aggregate traffic per minute\n",
    "    traffic_per_minute = df.groupby(df[\"Timestamp\"].dt.floor(\"min\")).size().reset_index(name=\"Minute_Request_Count\")\n",
    "\n",
    "    # Aggregate traffic per hour\n",
    "    traffic_per_hour = df.groupby(df[\"Timestamp\"].dt.floor(\"h\")).size().reset_index(name=\"Hourly_Request_Count\")\n",
    "\n",
    "    # Aggregate traffic per day\n",
    "    traffic_per_day = df.groupby(df[\"Timestamp\"].dt.floor(\"D\")).size().reset_index(name=\"Daily_Request_Count\")\n",
    "    \n",
    "    return traffic_per_minute, traffic_per_hour, traffic_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth cell - Detect traffic anomalies\n",
    "def detect_traffic_anomalies(traffic_df, count_column):\n",
    "    \"\"\"\n",
    "    Detect anomalies in traffic using Z-score method\n",
    "    \"\"\"\n",
    "    # Calculate Z-scores\n",
    "    traffic_df[\"Z_Score\"] = (traffic_df[count_column] - traffic_df[count_column].mean()) / traffic_df[count_column].std()\n",
    "    \n",
    "    # Flag anomalies where Z-score > 3\n",
    "    traffic_df[\"Anomaly\"] = np.abs(traffic_df[\"Z_Score\"]) > 3\n",
    "    \n",
    "    return traffic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f7699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth cell - Generate plot functions that save to files rather than displaying\n",
    "def generate_traffic_plots(traffic_per_minute, traffic_per_hour, traffic_per_day, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Generate traffic plots and save as files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Plot Traffic Per Minute\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(traffic_per_minute[\"Timestamp\"], traffic_per_minute[\"Minute_Request_Count\"], color=\"blue\", label=\"Minute Traffic\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Requests per Minute\")\n",
    "    plt.title(\"Website Traffic Per Minute\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{output_dir}/minute_traffic.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Traffic Per Hour\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(traffic_per_hour[\"Timestamp\"], traffic_per_hour[\"Hourly_Request_Count\"], color=\"purple\", marker='o', linestyle='-', label=\"Hourly Traffic\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Requests per Hour\")\n",
    "    plt.title(\"Website Traffic Per Hour\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{output_dir}/hour_traffic.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Traffic Per Day\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(traffic_per_day[\"Timestamp\"], traffic_per_day[\"Daily_Request_Count\"], color=\"orange\", marker='o', linestyle='-', label=\"Daily Traffic\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Requests per Day\")\n",
    "    plt.title(\"Website Traffic Per Day\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{output_dir}/day_traffic.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Traffic plots saved to {output_dir} directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b7a1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sixth cell - Generate anomaly plots\n",
    "def generate_anomaly_plots(traffic_per_minute, traffic_per_hour, traffic_per_day, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Generate anomaly plots and save as files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    def plot_anomalies(df, count_column, title, filename):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.scatter(df[\"Timestamp\"], df[count_column], c=df[\"Anomaly\"].astype(int), cmap=\"coolwarm\", label=\"Requests\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(count_column)\n",
    "        plt.title(title)\n",
    "        plt.colorbar(label=\"Anomaly\")\n",
    "        plt.savefig(f\"{output_dir}/{filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot anomalies\n",
    "    plot_anomalies(traffic_per_minute, \"Minute_Request_Count\", \"Minute-Level Traffic Anomalies\", \"minute_anomalies.png\")\n",
    "    plot_anomalies(traffic_per_hour, \"Hourly_Request_Count\", \"Hourly-Level Traffic Anomalies\", \"hour_anomalies.png\")\n",
    "    plot_anomalies(traffic_per_day, \"Daily_Request_Count\", \"Daily-Level Traffic Anomalies\", \"day_anomalies.png\")\n",
    "    \n",
    "    print(f\"Anomaly plots saved to {output_dir} directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "566f5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seventh cell - Detect packet anomalies using One-Class SVM\n",
    "def detect_packet_anomalies(df):\n",
    "    \"\"\"\n",
    "    Use One-Class SVM to detect packet-level anomalies\n",
    "    \"\"\"\n",
    "    # Feature Selection - Keep relevant numerical features\n",
    "    features = ['Length']\n",
    "    if 'Time_Difference' in df.columns:\n",
    "        features.append('Time_Difference')\n",
    "\n",
    "    # Encode categorical features\n",
    "    categorical_cols = ['Source', 'Destination', 'Protocol']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = LabelEncoder().fit_transform(df[col])\n",
    "            features.append(col)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[features])\n",
    "\n",
    "    # Train One-Class SVM model\n",
    "    svm_model = OneClassSVM(kernel='rbf', gamma='auto', nu=0.01)\n",
    "    svm_model.fit(X)\n",
    "\n",
    "    # Predict anomalies\n",
    "    df['Anomaly'] = svm_model.predict(X)\n",
    "    df['Anomaly'] = df['Anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
    "    \n",
    "    # Save the SVM model for future use\n",
    "    pickle.dump(svm_model, open(\"svm_anomaly_model.pkl\", \"wb\"))\n",
    "    pickle.dump(scaler, open(\"feature_scaler.pkl\", \"wb\"))\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fdd3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize anomalies - Improved Dynamic Version\n",
    "def categorize_anomalies(df):\n",
    "    \"\"\"\n",
    "    Categorize detected anomalies dynamically based on dataset size.\n",
    "    \"\"\"\n",
    "    # Categorize based on Length\n",
    "    def categorize_length(length):\n",
    "        if length > 1000:\n",
    "            return \"Large Packet\"\n",
    "        elif length < 50:\n",
    "            return \"Small Packet\"\n",
    "        else:\n",
    "            return \"Normal Size\"\n",
    "\n",
    "    df[\"Anomaly_Category\"] = df[\"Length\"].apply(categorize_length)\n",
    "\n",
    "    # Categorize based on Protocol\n",
    "    protocol_mapping = {\n",
    "        6: \"TCP\",\n",
    "        17: \"UDP\",\n",
    "        1: \"ICMP\",\n",
    "        9: \"ARP\",\n",
    "        303: \"Unknown/Custom\"\n",
    "    }\n",
    "    df[\"Protocol_Category\"] = df[\"Protocol\"].map(protocol_mapping).fillna(\"Other\")\n",
    "\n",
    "    # Categorize based on Request Frequency\n",
    "    # Dynamic threshold: 2% of total records or minimum 5\n",
    "    dynamic_threshold = max(5, int(len(df) * 0.05))\n",
    "    print(f\"Dynamic bot detection threshold set to {dynamic_threshold} requests.\")\n",
    "\n",
    "    # Calculate request frequency per source\n",
    "    df[\"Request_Frequency\"] = df[\"Source\"].map(df[\"Source\"].value_counts())\n",
    "    df[\"Source_Category\"] = df[\"Request_Frequency\"].apply(\n",
    "        lambda x: \"Bot Suspect\" if x > dynamic_threshold else \"Normal\"\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "178c049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bot_suspects(df):\n",
    "    \"\"\"\n",
    "    Identify bot suspects based on request behavior\n",
    "    \"\"\"\n",
    "    if 'Time_Difference' not in df.columns or 'Source' not in df.columns:\n",
    "        print(\"Required columns missing for bot detection.\")\n",
    "        return df\n",
    "\n",
    "    # Fast repeated requests (e.g., requests within <0.5 seconds)\n",
    "    df[\"Fast_Request\"] = df[\"Time_Difference\"] < 0.5\n",
    "\n",
    "    # Group by Source and count fast requests\n",
    "    fast_request_counts = df.groupby(\"Source\")[\"Fast_Request\"].sum().reset_index(name=\"Fast_Requests\")\n",
    "\n",
    "    # Threshold: Sources with a lot of fast requests are suspected bots\n",
    "    suspect_sources = fast_request_counts[fast_request_counts[\"Fast_Requests\"] > 50][\"Source\"]\n",
    "\n",
    "    # Mark in the main dataframe\n",
    "    df[\"Bot_Suspect\"] = df[\"Source\"].isin(suspect_sources)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "325b6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ninth cell - Generate anomaly report\n",
    "def generate_anomaly_report(df, output_dir=\"reports\"):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive anomaly report\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get outliers only\n",
    "    outliers = df[df['Anomaly'] == 1]\n",
    "    \n",
    "    # Save outliers and full report\n",
    "    outliers.to_csv(f\"{output_dir}/outliers_detected.csv\", index=False)\n",
    "    df.to_csv(f\"{output_dir}/final_anomalies_report.csv\", index=False)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    anomaly_percentage = df[\"Anomaly\"].mean() * 100\n",
    "    print(f\"Detected {anomaly_percentage:.2f}% of traffic as anomalies.\")\n",
    "    \n",
    "    # Anomaly summary\n",
    "    anomaly_summary = {\n",
    "        \"total_records\": len(df),\n",
    "        \"anomalies_detected\": int(df[\"Anomaly\"].sum()),\n",
    "        \"anomaly_percentage\": anomaly_percentage,\n",
    "        \"anomaly_categories\": outliers[\"Anomaly_Category\"].value_counts().to_dict(),\n",
    "        \"protocol_categories\": outliers[\"Protocol_Category\"].value_counts().to_dict(),\n",
    "        \"source_categories\": outliers[\"Source_Category\"].value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    # Save summary as JSON\n",
    "    with open(f\"{output_dir}/anomaly_summary.json\", \"w\") as f:\n",
    "        import json\n",
    "        json.dump(anomaly_summary, f, indent=4)\n",
    "    \n",
    "    print(f\"Reports saved to {output_dir} directory\")\n",
    "    return anomaly_summary, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1baadf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenth cell - Main function to process data\n",
    "def main(file_path, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Main function to process the data and generate results\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    plots_dir = f\"{output_dir}/plots\"\n",
    "    reports_dir = f\"{output_dir}/reports\"\n",
    "    \n",
    "    # Process the uploaded file\n",
    "    df = process_traffic_data(file_path)\n",
    "    \n",
    "    # Generate traffic analysis\n",
    "    traffic_per_minute, traffic_per_hour, traffic_per_day = generate_traffic_analysis(df)\n",
    "    \n",
    "    # Detect anomalies in traffic\n",
    "    traffic_per_minute = detect_traffic_anomalies(traffic_per_minute, \"Minute_Request_Count\")\n",
    "    traffic_per_hour = detect_traffic_anomalies(traffic_per_hour, \"Hourly_Request_Count\")\n",
    "    traffic_per_day = detect_traffic_anomalies(traffic_per_day, \"Daily_Request_Count\")\n",
    "    \n",
    "    # Generate traffic plots\n",
    "    generate_traffic_plots(traffic_per_minute, traffic_per_hour, traffic_per_day, plots_dir)\n",
    "    \n",
    "    # Generate anomaly plots\n",
    "    generate_anomaly_plots(traffic_per_minute, traffic_per_hour, traffic_per_day, plots_dir)\n",
    "    \n",
    "    # Detect packet-level anomalies\n",
    "    df, features_used = detect_packet_anomalies(df)\n",
    "    \n",
    "    # Categorize anomalies\n",
    "    df = categorize_anomalies(df)\n",
    "\n",
    "    df = identify_bot_suspects(df)\n",
    "\n",
    "    # Generate anomaly report\n",
    "    anomaly_summary, outliers = generate_anomaly_report(df, reports_dir)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Results saved to {output_dir} directory\")\n",
    "    \n",
    "    return {\n",
    "        \"processed_data\": df,\n",
    "        \"traffic_analysis\": {\n",
    "            \"minute\": traffic_per_minute,\n",
    "            \"hour\": traffic_per_hour,\n",
    "            \"day\": traffic_per_day\n",
    "        },\n",
    "        \"anomaly_summary\": anomaly_summary,\n",
    "        \"outliers\": outliers\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f403d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'anomalies_dataset.csv' not found. Please provide a valid file path.\n"
     ]
    }
   ],
   "source": [
    "# Eleventh cell - Example usage (when running in Jupyter)\n",
    "# Run this cell to test the functionality\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # For testing with a sample file\n",
    "    # Replace \"sample_traffic.csv\" with your test file\n",
    "    sample_file = \"anomalies_dataset.csv\"\n",
    "    \n",
    "    if os.path.exists(sample_file):\n",
    "        results = main(sample_file)\n",
    "        print(\"Analysis completed successfully!\")\n",
    "    else:\n",
    "        print(f\"File '{sample_file}' not found. Please provide a valid file path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
